# -*- coding: utf-8 -*-
"""Cab_fare_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1owjqZf3RwqNYNuI5tJexaz_Ir5adUBNt
"""

#!pip install fancyimpute

"""## **Setting** **Environment**"""

#library lading for dataset 
import pandas as pd
#library for performing numerical operation
import numpy as np
#library for visualization
import matplotlib.pyplot as plt
#library for visualization
import seaborn as sns
#library for counting variables
from collections import Counter
#load the libraries for ML algorithm
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
#for modeling train and test split
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
#for checking accuracy metrics in regression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
#for missing values treatment
from fancyimpute import KNN
#save the model file into pickle file 
import joblib
from sys import argv

#for making an inline graph display 
#get_ipython().run_line_magic('matplotlib','inline')

train_path  = argv[1]
test_path   = argv[2] 

#load the data 
train = pd.read_csv(train_path, sep=',')
test = pd.read_csv(test_path, sep=',')

"""## **DATA** **Explortion**"""
print("-----------------------")
print("Data Exploration Starts")
print("-----------------------")
#understanding the training data
train.describe()

train.head()

"""Info: Training data contains missing values in **passenger_count**"""

#understanding test data 
test.describe()

"""Info: Test data is fine and not having any null values"""

#check the dimention of data in both train and test dataset
print("Shape of training dataset : ", train.shape)
print("Shape of test dataset : ", test.shape)

#check which type of data present in the dataset with their types
#train.info()

"""Info: Here we can see fare_amount and pickup_datetime is in object type so we need to convert it into respective types float and datetime object"""

#check the type of data present in test dataset with their types
#test.info()

"""Info: Here we can see pickup_datetime is in object type so we need to convert it into datetime object"""

#test.head()

#convert fare_amount to numeric in train data
train['fare_amount'] = pd.to_numeric(train['fare_amount'],errors='coerce')

#convert the pickup_datetime object to timestamp dtypes 
train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'],errors = 'coerce', format='%Y-%m-%d %H:%M:%S UTC')

#check dataype changed or not
#train.dtypes

"""# #General assumption related for Cab fare prediction as per dataset

1.   Passanger count in cab for hatchbag and suv is MAX 6
2.   Cab fare values cannot be -ve and cannot be charged high for less distance

3.   Longitudes valid range from -180 to +180 
4.   Latitudes valid range from -90 to +90

#Explore the column data
"""

#seperate the passenger count column as its not continuous and making it categorical
cat_var = 'passenger_count'
numerical_var = ['fare_amount','pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']

#setting sns library for plots
#sns.set(style= 'darkgrid', palette='Set2')

#count plot on passenger count
#plt.figure(figsize=(15,8))
#sns.countplot(x="passenger_count", data=train)

#vdf = sns.load_dataset(train)
#sns.set_style("darkgrid")
#sns.pairplot(train[numerical_var], diag_kind='kde',kind='scatter',palette='hls')
#plt.show()

#checking relationship between passenger and fare amount
#plt.figure(figsize=(15,8))
#plt.boxplot(train['fare_amount'])
#plt.show()

#Note: Due to huge outlier difference the box plot is not visible now we first remove the outliers and normalize the data as we have seen in scatterplot

"""# Fare amount cannot be negative value, that doesn't make sense so will remove data having less than 0 fare amount"""

#check how many rows having -ve values of fareamount
#sum(train['fare_amount']<1)
#print(train[train['fare_amount']<1])

#remove this rows from training dataset 
train = train.drop(train[train['fare_amount']<1].index, axis=0)

print("shape of Train dataset", train.shape)

"""# Check passenger count as per assumption we will check max 6 passenger rest we will remove as for Hatchbag and suv max capacity is 6 members"""

#finding count based on number of passenger in cab
#for i in range(4,15):
#  print("passenger count "+str(i)+" is ",sum(train['passenger_count']>i))

#we can see that 20 obersations we are getting from 6 so rest we will not consider
#train[train['passenger_count']>6]

#sum(train['passenger_count']>6)

#will check the data for passenger_count less than 1 as it doesn't make sense
#train[train['passenger_count']<1]

#sum(train['passenger_count']<1)

"""Info: As we can we have 58 rows which is having passenger count is less than 1 so we will remove those columns

"""

#lets confirm from test data also for passenger count unique values
#test['passenger_count'].value_counts()

#From above results we can conclude that 20 rows of high numbers of passenger count and 58 rows of zero passenger count doesn't fit good for analysis as it contains outliers and false data
train = train.drop(train[train['passenger_count']>6].index, axis=0)
train = train.drop(train[train['passenger_count']<1].index, axis=0)

print("shape of Train dataset",train.shape)

"""# 3rd part latitude and longitudes valid ranges"""

#check for valid range in each column of data
print("pickup_Longitude above 180 is", sum(train['pickup_longitude']>180))
print("pickup_Longitude below -180 is", sum(train['pickup_longitude']<-180))

print("pickup_Latitude above 90 is", sum(train['pickup_latitude']>90))
print("pickup_Latitude below -90 is", sum(train['pickup_latitude']<-90))

print("dropoff_Longitude above 180 is", sum(train['dropoff_longitude']>180))
print("dropoff_Longitude below -180 is", sum(train['dropoff_longitude']<-180))

print("dropoff_Latitude above 90 is", sum(train['dropoff_latitude']>90))
print("dropoff_Latitude below -90 is", sum(train['dropoff_latitude']<-90))

#printing min and max values in location related data
print(min(train['pickup_longitude']),'\t', max(train['pickup_longitude']))
print(min(train['pickup_latitude']),'\t',max(train['pickup_latitude']))
print(min(train['dropoff_longitude']),'\t',max(train['dropoff_longitude']))
print(min(train['dropoff_latitude']),'\t',max(train['dropoff_latitude']))

"""Info: Above result shows only 1 data point is out of range so we will remove that data point to avoid skewness in data"""

train = train.drop(train[train['pickup_latitude']>90].index, axis=0)

print("shape of Train dataset", train.shape)

#We need to check for zero values in latitudes and longitudes
#for i in numerical_var[1:]:
#  print(i,"equal to 0 is",sum(train[i]==0))

#we need to remove all location related data which contains zero value
for i in numerical_var[1:]:
  #print(i,"equal to 0 is",sum(train[i]==0))
  train = train.drop(train[train[i] == 0].index, axis=0)

print("shape of Train dataset", train.shape)

#plot the graph for fare amount feature
#plt.figure(figsize=(15,8))
#sns.countplot(x=train['fare_amount'], data=train,  color='salmon')

#save the preprocessed version 1 for training file 
df_version1 = train.copy()

"""# so far we removed data on asumptions and we have normalised data of (15661,7) from (16067,7)

## **Missing Value Treatment**
"""
print("----------------------------------------------------")
print("Missing Value Exploration & Outlier Treatment Starts")
print("----------------------------------------------------")
#check for missing values in train data
#train.isna().sum()

#test.isna().sum()

#checking rows for all null data in training dataset
null_data = train[train.isnull().any(axis=1)]
print("Null Data", null_data)

#Checking which row having null in the pickup_datetime column
#train[train['pickup_datetime'].isnull()]

#remove the row which having null value axis = 0 means removing row
train = train.drop(train[train['pickup_datetime'].isnull()].index, axis= 0) 
#check row deleted or not if isnull count = 0 
print(train['pickup_datetime'].isnull().sum())

print("shape of Train dataset", train.shape)

#create dataframe for missing percentage 
missing_values = pd.DataFrame(train.isnull().sum())
#setting index to data frame
missing_values = missing_values.reset_index()
#missing_values

#Show the missing % of data for columns
missing_values = missing_values.rename(columns={'index':'Variables', 0 : 'Missing_%'})
#missing_values

#calculate persentage
missing_values['Missing_%'] = (missing_values['Missing_%']/len(train))*100
missing_values = missing_values.sort_values('Missing_%', ascending=False).reset_index(drop=True)
#missing_values

#missing_values.columns

#plot the graph for missing vaues 
#plt.figure(figsize=(15,5))
#sns.barplot(x=missing_values['Variables'], y = missing_values['Missing_%'], data= missing_values)
#plt.show()

#train['fare_amount'].describe()

#fill the missing values of rows with imputation technique we are not using mean and mode as data is biased
#using  KNN for imputation method with k=19(assumption)
#train.std()

#sepeate the datetime columns from dataframe and save for later
pickup_datetime = pd.DataFrame(train['pickup_datetime'])

columns = list(train.columns)
columns.pop(1)
#columns

#Impute the missing values using KNN Technique for rows
from fancyimpute import KNN
train = pd.DataFrame(KNN(k=19).fit_transform(train.drop('pickup_datetime',axis=1)),columns=columns, index=train.index)

#confirmation for missing values resolved
#train.isna().sum()

"""## Outlier Treatement for training dataset"""

#comparing std deviation before and after imputation we can see values are matching 
#train.std()

"""Info: we can see only fare amount data is having high standard deviation hence we need to remove outliers from training dataset"""

#train['fare_amount'].describe()

#for fare amount data is having more deviated top 20 highest values
#train['fare_amount'].sort_values(ascending=False)[:20]

#for fare amount data is having more deviated top 20 lowest values
#train['fare_amount'].sort_values(ascending=True)[:20]

#from above observation we can see Rs. 54343.00 ,4343.00 fareamount is extreme outlier

#drop the fare amount data which is having higher fare than 454 
train = train.drop(train[train['fare_amount']> 454].index, axis=0)

print("shape of Train dataset", train.shape)

#save the dataframe as 2nd version of outlier_treatment
df_version2 = pd.merge(pickup_datetime, train, right_index=True, left_index=True)

#train.std()

#plt.figure(figsize=(20,5))
#plt.xlim(0,200)
#sns.boxplot(x=train['fare_amount'], data=train, orient='h')
#plt.title('Boxplot of fare amount')
#plt.show()

#plt.figure(figsize=(20,10))
#plt.xlim(0,100)
#sns.boxplot(x=train['fare_amount'], y=train['passenger_count'], data=train, orient='h')
#plt.title('Boxplot of fare amount vs passenger count')
#plt.show()

#converting passenger count from float dtype to categorical element 
train['passenger_count'] = train['passenger_count'].round().astype('object').astype('category')

#plt.figure(figsize=(20,10))
#plt.xlim(0,100)
#sns.boxplot(x=train['fare_amount'], y=train['passenger_count'], data=train, orient='h')
#plt.title('Boxplot of fare amount vs passenger count')
#plt.show()

#merge the pickupdatetime column with other data after imputation
train1 = pd.merge(pickup_datetime, train, right_index=True, left_index=True)
#train1.head()
imputed_df = train1
imputed_df.to_csv("D:/edwisor_details/project/final_project_submission/output_files/imputed_data.csv", index=False)

#train.describe()

"""Info :#we can see standard deviation in fare_amount is more to need to finetune it

## Feature Engineering for columns pickup_datetime
"""
print("--------------------------")
print("Feature Engineering Starts")
print("--------------------------")

#continue working on df version2 and reassign to train 
train = df_version2
print("shape of Train dataset", train.shape)

#train.head(10)

#convert the pickup_datetime object to timestamp dtypes 
test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'],errors = 'coerce', format='%Y-%m-%d %H:%M:%S UTC')

#test.info()

#from pickup_datetime we will create the year, month, date, day, hour, minute
datasets = [train, test]
for t_set in datasets:
  #print(t_set)
  t_set['year'] = t_set['pickup_datetime'].apply(lambda row: row.year)
  t_set['month'] = t_set['pickup_datetime'].apply(lambda row: row.month)
  t_set['date'] = t_set['pickup_datetime'].apply(lambda row: row.day)
  t_set['day'] = t_set['pickup_datetime'].apply(lambda row: row.dayofweek)
  t_set['hour'] = t_set['pickup_datetime'].apply(lambda row: row.hour)
  t_set['minute'] = t_set['pickup_datetime'].apply(lambda row: row.minute)

#train.info()

print("shape of Test dataset", test.shape)

#train.head()

#converting passenger count from float dtype to categorical element 
test['passenger_count'] = test['passenger_count'].round().astype('object').astype('category')

#test.head()

#Calculate the distance from longitude and latitude columns and create the distance column
#for working on geo coordinates we have a function called haversine using that we create another variable called distance
from math import radians, cos, sin, asin, sqrt

def haversine(var):
  """
  Calculate the greate circle distance between two points on the earth(speicified in decimal degree)
  """
  long1 = var[0]
  latt1 = var[1]
  long2 = var[2]
  latt2 = var[3]

  #convert the decimals to radians
  long1, latt1, long2, latt2 = map(radians, [long1, latt1, long2, latt2])

  #formula
  dlong = long2 - long1
  dlatt = latt2 - latt1
  d_convertion = sin(dlatt/2)**2 + cos(latt1) * cos(latt2) * sin(dlong/2)**2
  cir = 2 * asin(sqrt(d_convertion))
  #radius of earth in km is 6371
  km = 6371 * cir
  return km

#add distance column  to train and test data and apply haversine function to columns
train['distance'] = train[['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].apply(haversine,axis=1)

test['distance'] = test[['pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].apply(haversine,axis=1)

print("shape of Train dataset", train.shape)
print("shape of Test dataset", test.shape)

#train.head()

#test.head()

#check the unique values into data
#train.nunique()

#test.nunique()

#validate the distance column with its min and maximum range upto 20
#train['distance'].sort_values(ascending=False)[:20]

#train['distance'].sort_values(ascending=True)[:20]

#As we can see that from above 2 observation is distance with 0 doesnt make any sense and distance which is more than 4000km is not possible in 1 trip
#so we will remove those data to make our data free from outliers

#check the count of zero distance rows
#Counter(train['distance'] ==0)

#check the count of zero distance rows
#Counter(test['distance'] ==0)

#check the count of km > 130 distance rows
#Counter(train['distance'] > 130)

#check the count of km > 130 distance rows
#Counter(test['distance'] > 130)

#so we will remove total 155+2 = 157 rows as a part of outlier treatment
train = train.drop(train[train['distance'] == 0].index, axis = 0)
train = train.drop(train[train['distance'] > 130].index, axis = 0)

print("shape of Train dataset", train.shape)

#Now we have seprate data for pickup_datetime so we will remove and all columns related to longitudes and latitudes are not further required
drop_column_set = ['pickup_datetime', 'pickup_longitude','pickup_latitude', 'dropoff_longitude', 'dropoff_latitude' ]
train = train.drop(drop_column_set, axis=1)

#train.head()

#train.info()

#converting back passengercount variables into int in both train and test
train['passenger_count'] = train['passenger_count'].astype('int64')
test['passenger_count'] = test['passenger_count'].astype('int64')

#train.dtypes

#drop the columns which is not important after feature Engineering
test = test.drop(drop_column_set, axis=1)

#test.head()

#test.dtypes

"""## DATA Visualization"""

#visualize the passenger count
# plt.figure(figsize=(15,8))
# sns.countplot(x="passenger_count", data=train)

# #relationship between passenger count and fare
# plt.figure(figsize=(15,7))
# plt.scatter(x=train['passenger_count'], y=train['fare_amount'], s=10)
# plt.xlabel('No. of Passengers')
# plt.ylabel('Fare Amount')
# plt.show()

#From above 2 visualization we can say that
#1) single travelling passengers are most frequent passengers
#2) highest fare coming from single and double no of passengers

#countplots on year, month, day of week, hour to understand where we have most peak
# plt.figure(figsize=(15,10))
# sns.countplot(train['year'])

# plt.figure(figsize=(15,10))
# sns.countplot(train['month'])

# plt.figure(figsize=(15,10))
# sns.countplot(train['day'])

# plt.figure(figsize=(15,10))
# sns.countplot(train['hour'])

#observations from above graphs 
#1) utlization of service from year 2014 seems downfall
#2) 3rd quarter of service is affected than other quarters
#3) weekday utilization of service is more as compared to weekend
#4) cab service utilization is more in night time as compared to day time

#visualize relation between distance and fare
# plt.figure(figsize=(15,8))
# plt.scatter(x = train['distance'],y = train['fare_amount'], c = "b")
# plt.xlabel('Distance in Km')
# plt.ylabel('Fare Amount')
# plt.show()

#this graph clearly shows the effect or amount on fare

#check correlation in data
# plt.figure(figsize=(15,10))
# heatmap_ = sns.heatmap(train.corr(), square=True, cmap='RdYlGn', annot=True)
# plt.title("Correlation Matrix")
# plt.show()

#from above correlation diagram we can clearly see that distance and fare is correlated

"""# Feature Scaling"""

#check the training dat how its distributed

# for i in ['fare_amount', 'distance']:
    # print(i)
    # sns.distplot(train[i],bins='auto',color='blue')
    # plt.title("Distribution for Variable "+i)
    # plt.ylabel("Density")
    # plt.show()

#here we can see that target variable is skewed hence we need to transform to reduce skewness

train['fare_amount'] = np.log1p(train['fare_amount'])
train['distance'] = np.log1p(train['distance'])

#check for log converted distribution
# for i in ['fare_amount', 'distance']:
    # print(i)
    # sns.distplot(train[i],bins='auto',color='blue')
    # plt.title("log Distribution for Variable "+i)
    # plt.ylabel("Density")
    # plt.show()

#so we can see bell curve which looks like normal distribution hence we will not do any further scaling

#same we will apply for test data also
# sns.distplot(test['distance'],bins='auto',color='red')
# plt.title("Distribution for Variable "+i)
# plt.ylabel("Density")
# plt.show()

#since skewness of distance variable is high we will apply log transform
test['distance'] = np.log1p(test['distance'])

#after transform check 
# sns.distplot(test['distance'],bins='auto',color='red')
# plt.title("log Distribution for Variable "+i)
# plt.ylabel("Density")
# plt.show()

#final data for model 
print("Final Data for model", train.head())

"""# **Apply Machine learning Regression Algorithms**

Info : 
*   x_train, y_train are train subsets
*   x_test, y_test are validation subsets
"""
print("---------------------")
print("Model Building Starts")
print("---------------------")

##train and test split for  modelling
x_train, x_test, y_train, y_test = train_test_split( train.iloc[:, train.columns != 'fare_amount'], train.iloc[:, 0], test_size = 0.25, random_state = 1)

print("x_train ", x_train.shape)
print("x_test", x_test.shape)

"""# Linear Regression Model"""

#building model on training dataset
#LR_fit = LinearRegression().fit(x_train, y_train)

#predict on train data
#prediction_train_LR = LR_fit.predict(x_train)

#predict on test data
#prediction_test_LR = LR_fit.predict(x_test)

#metric calculation RMSE for test data
##calculating RMSE for test data
#RMSE_test_LR = np.sqrt(mean_squared_error(y_test, prediction_test_LR))
#print("Root Mean Squared Error For Test data = "+str(RMSE_test_LR))

##calculating RMSE for train data
#RMSE_train_LR= np.sqrt(mean_squared_error(y_train, prediction_train_LR))

#print("Root Mean Squared Error For Training data = "+str(RMSE_train_LR))

#calculate R^2 for train data
from sklearn.metrics import r2_score
#r2_score(y_train, prediction_train_LR)

#calculate R^2 for test data
#r2_score(y_test, prediction_test_LR)

"""**Another Approach for LR with hypertuning**"""

X = train.drop('fare_amount', axis=1).values
y = train['fare_amount'].values

# # Setup the parameters and distributions to sample from: param_grid
# param_grid = {'copy_X':[True, False],
          # 'fit_intercept':[True,False]}
# # Instantiate a Decision reg classifier: Lregg
# Lregg = LinearRegression()

# # Instantiate the gridSearchCV object: Lregg_cv
# Lregg_cv = GridSearchCV(Lregg, param_grid, cv=5,scoring='r2')

# # Fit it to the data
# Lregg_cv.fit(X, y)

# Print the tuned parameters and score
# print("Tuned Decision reg Parameters: {}".format(Lregg_cv.best_params_))
# print("Best score is {}".format(Lregg_cv.best_score_))

# # Create the regressor: reg_all
# reg_all = LinearRegression(copy_X= True, fit_intercept=True)

# # Fit the regressor to the training data
# reg_all.fit(x_train,y_train)

# # Predict on the test data: y_pred
# y_pred = reg_all.predict(x_test)

# # Compute and print R^2 and RMSE
# print("R^2: {}".format(reg_all.score(x_test, y_test)))
# rmse = np.sqrt(mean_squared_error(y_test,y_pred))
# print("Root Mean Squared Error: {}".format(rmse))
# print("----------------------------")
# test_scores(reg_all)



"""#Random Forest Regression"""

# Create the random grid
from sklearn.model_selection import RandomizedSearchCV
random_grid = {'n_estimators': range(100,500,100),
               'max_depth': range(5,20,1),
               'min_samples_leaf':range(2,5,1),
                'max_features':['auto','sqrt','log2'],
              'bootstrap': [True, False],
              'min_samples_split': range(2,5,1)}
# Instantiate a Decision Forest classifier: Forest
RForest = RandomForestRegressor()

# Instantiate the gridSearchCV object: Forest_cv
RForest_cv = RandomizedSearchCV(RForest, random_grid, cv=5)

# Fit it to the data
RForest_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Random Forest Parameters: {}".format(RForest_cv.best_params_))
print("Best score is {}".format(RForest_cv.best_score_))

#supporting metric function
from sklearn import metrics
def scores(y, y_):
    print('r2 square  ', metrics.r2_score(y, y_))
    print('Adjusted r square:{}'.format(1 - (1-metrics.r2_score(y, y_))*(len(y)-1)/(len(y)-x_train.shape[1]-1)))
    print('MAPE:{}'.format(np.mean(np.abs((y - y_) / y))*100))
    print('MSE:', metrics.mean_squared_error(y, y_))
    print('RMSE:', np.sqrt(metrics.mean_squared_error(y, y_))) 

def test_scores(model):
    print('<<<------------------- Training Data Score --------------------->>>')
    print()
    #Predicting result on Training data
    y_pred = model.predict(x_train)
    scores(y_train,y_pred)
    print()
    print('<<<------------------- Test Data Score --------------------->>>')
    print()
    # Evaluating on Test Set
    y_pred = model.predict(x_test)
    scores(y_test,y_pred)

# Instantiate a Forest regressor: Forest
RForest = RandomForestRegressor(n_estimators=400, min_samples_split= 2, min_samples_leaf=4, max_features='log2', max_depth=15, bootstrap=False)

# Fit the regressor to the data
RForest.fit(x_train,y_train)

# Compute and print the coefficients
RForest_features = RForest.feature_importances_
print(RForest_features)

# Sort feature importances in descending order
indices = np.argsort(RForest_features)[::1]

# Rearrange feature names so they match the sorted feature importances
names = [test.columns[i] for i in indices]

# Creating plot
#fig = plt.figure(figsize=(20,10))
#plt.title("Feature Importance")

# Add horizontal bars
# plt.barh(range(pd.DataFrame(x_train).shape[1]),RForest_features[indices],align = 'center')
# plt.yticks(range(pd.DataFrame(x_train).shape[1]), names)
# plt.savefig('Random forest feature importance')
# plt.show()# Make predictions

test_scores(RForest)

"""# XG Boost Regression"""

# from xgboost import XGBRegressor
# import xgboost as xgb
# #convert the data into matrix form
# data_matrix = xgb.DMatrix(data=X, label=y)
# dtrain = xgb.DMatrix(x_train, label=y_train)
# dtest = xgb.DMatrix(x_test)

# dtrain,dtest,data_matrix

# #Instantiate the xgboost Regressor
# params = {"objective":"reg:linear",'colsample_bytree': 0.3,'learning_rate': 0.1,
                # 'max_depth': 5, 'alpha': 10}

# cv_results = xgb.cv(dtrain=data_matrix, params=params, nfold=5,
                    # num_boost_round=50,early_stopping_rounds=10,metrics="rmse", as_pandas=True, seed=123)
# cv_results.head()

# #without hypertuning check the result
# Xgb = XGBRegressor()
# Xgb.fit(x_train,y_train)
# # pred_xgb = model_xgb.predict(X_test)
# test_scores(Xgb)

# #hypertune the XGboost
# # Create the random grid
# random_grid = {'n_estimators': range(100,500,100),
               # 'max_depth': range(3,10,1),
        # 'reg_alpha':np.logspace(-4, 0, 50),
        # 'subsample': np.arange(0.1,1,0.2),
        # 'colsample_bytree': np.arange(0.1,1,0.2),
        # 'colsample_bylevel': np.arange(0.1,1,0.2),
        # 'colsample_bynode': np.arange(0.1,1,0.2),
       # 'learning_rate': np.arange(.05, 1, .05)}
# # Instantiate a Decision Forest classifier: Forest
# Xgb = XGBRegressor()

# # Instantiate the gridSearchCV object: Forest_cv
# xgb_cv = RandomizedSearchCV(Xgb, random_grid, cv=5)

# # Fit it to the data
# xgb_cv.fit(X, y)

# # Print the tuned parameters and score
# print("Tuned Xgboost Parameters: {}".format(xgb_cv.best_params_))
# print("Best score is {}".format(xgb_cv.best_score_))

# # Instantiate a xgb regressor: xgb
# Xgb = XGBRegressor(subsample= 0.9000000000000001, reg_alpha= 0.32374575428176433, n_estimators= 300, max_depth= 5, learning_rate=0.1, colsample_bytree= 0.5000000000000001, colsample_bynode=0.30000000000000004, colsample_bylevel=0.9000000000000001)

# # Fit the regressor to the data
# Xgb.fit(x_train,y_train)

# # Compute and print the coefficients
# xgb_features = Xgb.feature_importances_
# print(xgb_features)

# # Sort feature importances in descending order
# indices = np.argsort(xgb_features)[::1]

# # Rearrange feature names so they match the sorted feature importances
# names = [test.columns[i] for i in indices]

# # Creating plot
# fig = plt.figure(figsize=(20,10))
# plt.title("Feature Importance")

# # Add horizontal bars
# plt.barh(range(pd.DataFrame(x_train).shape[1]),xgb_features[indices],align = 'center')
# plt.yticks(range(pd.DataFrame(x_train).shape[1]), names)
# plt.savefig(' xgb feature importance')
# plt.show()# Make predictions
# test_scores(Xgb)

"""# Prediction of Fare for Provided test data

Info: We will use Random Forest Regressor using Grid Search CV
"""
print("------------------------------")
print("Prediction on Test Data Starts")
print("------------------------------")
prediction_RF_test_DF = RForest.predict(test)

#prediction_RF_test_DF

test['predicted_fare'] = prediction_RF_test_DF

#test.head()
#save the predicition result to csv_file
test.to_csv('D:/edwisor_details/project/final_project_submission/output_files/final_prediction_test.csv')

#saving the model file in pickle file
joblib.dump(RForest, 'cab_fare_r_forest_model.pkl')

print(test.head())
#for loading the model for future sample test data use following command
#rforest_from_joblib = joblib.load('cab_fare_r_forest_model.pkl')